---
title: "Assignment2"
author: "jesper fischer ehmsen"
date: "2023-02-23"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(cmdstanr)

library(posterior)
library(bayesplot)
```

## R Markdown


```{r}
source("~/Advanced-cognitive-modeling/agents.R")

#different parameter settings for the game and agents:
#ntrials the number of times the agents play the game
ntrials = 2
#alpha1_l is the learning rate for the first agent (matcher) when losing
alpha1_l = 0
#alpha1_l is the learning rate for the first agent (matcher) when winning
alpha1_w = 0
#alpha2_l is the learning rate for the first agent (non-matcher) when losing
alpha2_l = 0
#alpha1_l is the learning rate for the first agent (non-matcher) when winning
alpha2_w = 0
#bias1 is the bias of the first participant to answer 1. That is if it is 1 then agent 1 will choose 1 on the first trial, if 0.5 he picks at random
bias1 = 0.2
#bias 2 is what bias1 is for agent2
bias2 = 0.8

#the incentive of the first agent do you play after your belief (0) or do you choose the oppisite of your belief (1)
incentive1 = 0
#the incentive of the second agent do you play after your belief (0) or do you choose the oppisite of your belief (1)
incentive2 = 0


#different agents:
#random bias:
#to initialize a random bias agent the learning rates for that agent just have to set to 0 and the bias is will then determine the bias of the agent:

#win stay lose shift (WSLS):
#to initialize a (WSLS) agent the two learning rates for the agent has to be set to 1.

#rescorla wagner(RW) learner with constant learning rate
#to initialize this RW agent the two learning rates for the agent just have to be equal

#rescorla wagner(RW) learner with different learning rate for wins and loses
#to initialize a RW agent the two learning rates for the agent just have to be different:



#make the agents play once (not included)
df = rw_vs_rw(ntrials = ntrials,
         alpha1_l = alpha1_l,
         alpha1_w = alpha1_w,
         alpha2_l = alpha2_l,
         alpha2_w = alpha2_w,
         bias1 = bias1,
         bias2 = bias2,
         incentive1 = incentive1,  # 0 doesn't care about ,  1 does the oppisite
         incentive2 = incentive2
         )

df = data.frame(df)


#lets fit in stan:

setwd("~/Advanced-cognitive-modeling/assignment2")
filemodel = "stan_models/bias_vs_bias.stan"

mod = cmdstan_model(filemodel)

data = list(rw1 = df$rw1, rw2 = df$rw2, n = nrow(df))


fit <- mod$sample(
  data = data, 
  seed = 123, 
  chains = 4, 
  parallel_chains = 4,
  refresh = 500
)

fit
```



```{r}
source("~/Advanced-cognitive-modeling/stan_helpers.R")
library(ggdist)
#parameter recovery
pr = parameter_recovery_bias_vs_bias(10,30)

pr$plot
```







```{r}
#RB vs RW
source("~/Advanced-cognitive-modeling/agents.R")

#different parameter settings for the game and agents:
#ntrials the number of times the agents play the game
ntrials = 120
#alpha1_l is the learning rate for the first agent (matcher) when losing
alpha1_l = 0.9
#alpha1_l is the learning rate for the first agent (matcher) when winning
alpha1_w = 0.9
#alpha2_l is the learning rate for the first agent (non-matcher) when losing
alpha2_l = 0
#alpha1_l is the learning rate for the first agent (non-matcher) when winning
alpha2_w = 0
#bias1 is the bias of the first participant to answer 1. That is if it is 1 then agent 1 will choose 1 on the first trial, if 0.5 he picks at random
bias1 = 0.5
#bias 2 is what bias1 is for agent2
bias2 = 0.8

#the incentive of the first agent do you play after your belief (0) or do you choose the oppisite of your belief (1)
incentive1 = 0
#the incentive of the second agent do you play after your belief (0) or do you choose the oppisite of your belief (1)
incentive2 = 0


#different agents:
#random bias:
#to initialize a random bias agent the learning rates for that agent just have to set to 0 and the bias is will then determine the bias of the agent:

#win stay lose shift (WSLS):
#to initialize a (WSLS) agent the two learning rates for the agent has to be set to 1.

#rescorla wagner(RW) learner with constant learning rate
#to initialize this RW agent the two learning rates for the agent just have to be equal

#rescorla wagner(RW) learner with different learning rate for wins and loses
#to initialize a RW agent the two learning rates for the agent just have to be different:



#make the agents play once (not included)
df = rw_vs_rw(ntrials = ntrials,
         alpha1_l = alpha1_l,
         alpha1_w = alpha1_w,
         alpha2_l = alpha2_l,
         alpha2_w = alpha2_w,
         bias1 = bias1,
         bias2 = bias2,
         incentive1 = incentive1,  # 0 doesn't care about ,  1 does the oppisite
         incentive2 = incentive2
         )

df = data.frame(df)


#lets fit in stan:

setwd("~/Advanced-cognitive-modeling/assignment2")
filemodel = "stan_models/bias_vs_rw.stan"

mod = cmdstan_model(filemodel)

data = list(rw1 = df$rw1, rw2 = df$rw2, fb_rw1 = df$feedback_rw1, fb_rw2 = df$feedback_rw2, n = nrow(df))


fit <- mod$sample(
  data = data, 
  seed = 123, 
  chains = 4, 
  parallel_chains = 4,
  refresh = 500
)

fit$summary()


mcmc_hist(fit$draws("bias_1"))
mcmc_hist(fit$draws("alpha_1"))

mcmc_hist(fit$draws("bias_2"))
mcmc_hist(fit$draws("alpha_2"))

```


```{r}
source("~/Advanced-cognitive-modeling/stan_helpers.R")
library(ggdist)
#parameter recovery
pr = parameter_recovery_rw_vs_rw(5,20,0.3,0.7)

pr$plot
```



```{r}
#differing learning rates for winning and losing:



#Rw vs RW

#win stay lose shift agents:
source("~/Advanced-cognitive-modeling/agents.R")

#different parameter settings for the game and agents:
#ntrials the number of times the agents play the game
ntrials = 500
#alpha1_l is the learning rate for the first agent (matcher) when losing
alpha1_l = 0.8
#alpha1_l is the learning rate for the first agent (matcher) when winning
alpha1_w = 0.3
#alpha2_l is the learning rate for the first agent (non-matcher) when losing
alpha2_l = 0.2
#alpha1_l is the learning rate for the first agent (non-matcher) when winning
alpha2_w = 0.9
#bias1 is the bias of the first participant to answer 1. That is if it is 1 then agent 1 will choose 1 on the first trial, if 0.5 he picks at random
bias1 = 0.5
#bias 2 is what bias1 is for agent2
bias2 = 0.5

#the incentive of the first agent do you play after your belief (0) or do you choose the oppisite of your belief (1)
incentive1 = 0
#the incentive of the second agent do you play after your belief (0) or do you choose the oppisite of your belief (1)
incentive2 = 0


#different agents:
#random bias:
#to initialize a random bias agent the learning rates for that agent just have to set to 0 and the bias is will then determine the bias of the agent:

#win stay lose shift (WSLS):
#to initialize a (WSLS) agent the two learning rates for the agent has to be set to 1.

#rescorla wagner(RW) learner with constant learning rate
#to initialize this RW agent the two learning rates for the agent just have to be equal

#rescorla wagner(RW) learner with different learning rate for wins and loses
#to initialize a RW agent the two learning rates for the agent just have to be different:



#make the agents play once (not included)
df = rw_vs_rw(ntrials = ntrials,
         alpha1_l = alpha1_l,
         alpha1_w = alpha1_w,
         alpha2_l = alpha2_l,
         alpha2_w = alpha2_w,
         bias1 = bias1,
         bias2 = bias2,
         incentive1 = incentive1,  # 0 doesn't care about ,  1 does the oppisite
         incentive2 = incentive2
         )

df = data.frame(df)

#lets fit in stan:

setwd("~/Advanced-cognitive-modeling/assignment2")
filemodel = "stan_models/rw_win_lose_vs_rw.stan"

mod = cmdstan_model(filemodel)

data = list(rw1 = df$rw1, rw2 = df$rw2, fb_rw1 = df$feedback_rw1, fb_rw2 = df$feedback_rw2, n = nrow(df))


fit <- mod$sample(
  data = data, 
  seed = 123, 
  chains = 4, 
  parallel_chains = 4,
  refresh = 500
)


mcmc_hist(fit$draws("alpha_1l"))
mcmc_hist(fit$draws("alpha_1w"))


mcmc_hist(fit$draws("alpha_2l"))
mcmc_hist(fit$draws("alpha_2w"))
```


```{r}
#pr for that:
source("~/Advanced-cognitive-modeling/stan_helpers.R")

pr = parameter_recovery_rw_gen(ngames = 5,
                               start_trials = 100, 
                               end_trials = 110, 
                               lr1_l = 0.2, 
                               lr1_w = 0.8,
                               lr2_l = 0.9,
                               lr2_w = 0)
pr$plot

```



```{r}

library(ggtext)
#multilevel_model

#Plot1 (vs random bias)
#different parameter settings for the game and agents:
#ntrials the number of times the agents play the game
ntrials = 120
#alpha1_l is the learning rate for the first agent (matcher) when losing
alpha1_l_mu = 0.5
alpha1_l_sd = 0.1

alpha1_w_mu = 0.5
alpha1_w_sd = 0.1


#alpha2_l is the learning rate for the first agent (non-matcher) when losing
alpha2_l_mu = 0.4
alpha2_l_sd = 0.1



alpha2_w_mu = 0.4
alpha2_w_sd = 0.1

#bias1 is the bias of the first participant to answer 1. That is if it is 1 then agent 1 will choose 1 on the first trial, if 0.5 he picks at random
bias1 = 0.5
#bias 2 is what bias1 is for agent2
bias2 = 0.5

#the incentive of the first agent do you play after your belief (0) or do you choose the oppisite of your belief (1)
incentive1 = 0
#the incentive of the second agent do you play after your belief (0) or do you choose the oppisite of your belief (1)
incentive2 = 0

#make the agents play 100 times and plot it (included)
subjects = 5
source("~/Advanced-cognitive-modeling/agents.R")

agg = rw_vs_rw_hier(subjects = subjects,
         ntrials = ntrials,
         alpha1_l_mu,
         alpha1_l_sd,
         alpha1_w_mu,
         alpha1_w_sd,
         alpha2_l_mu,
         alpha2_l_sd,
         alpha2_w_mu,
         alpha2_w_sd,
         bias1, 
         bias2, 
         incentive1, 
         incentive2)

pairwise = agg$pairlevel
trialevel = agg$triallevel


setwd("~/Advanced-cognitive-modeling/assignment2")
filemodel = "stan_models/rw_vs_rw_hier.stan"

mod = cmdstan_model(filemodel)

trialevel$pair = as.factor(trialevel$pair)
trialevel$rw1 = as.factor(trialevel$rw1)

pairwise$pair = as.factor(pairwise$pair)
#plot them:
trialevel %>% mutate(x = rep(1:120,5)) %>% 
  pivot_longer(cols = c("expectation1","expectation2")) %>% inner_join(pairwise, by = "pair") %>% 
  ggplot() +
  geom_line(aes(x = x, y = value, col = name))+
  geom_richtext(aes(x = 30, y = 0, label = paste("alpha1 = ", round(alpha1_l,2))), size = 3)+
  geom_richtext(aes(x = 90, y = 0, label = paste("alpha2 = ", round(alpha2_l,2))), size = 3)+
  facet_wrap(~pair)+
  theme_classic()


```


```{r}
data = list(rw1 = as.matrix(agg$rw1), rw2 = as.matrix(agg$rw2), fb_rw1 = as.matrix(agg$rw1_fb), fb_rw2 = as.matrix(agg$rw2_fb), trials = nrow(agg$rw1), subjects = ncol(agg$rw1))


fit <- mod$sample(
  data = data, 
  seed = 123, 
  chains = 4, 
  parallel_chains = 4,
  refresh = 500
)



mcmc_hist(fit$draws("alpha_1_mu"))
mcmc_hist(fit$draws("alpha_1_sd"))

mcmc_hist(fit$draws("alpha_1"))


mcmc_hist(fit$draws("alpha_2_mu"))
mcmc_hist(fit$draws("alpha_2_sd"))

mcmc_hist(fit$draws("alpha_2"))
mcmc_hist(fit$draws("alpha_2w"))


mcmc_hist(fit$draws("bias_2_mu"))
```

















